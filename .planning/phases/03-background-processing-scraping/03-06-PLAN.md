---
phase: 03-background-processing-scraping
plan: 06
type: execute
wave: 1
depends_on: []
files_modified:
  - functions/src/schedulers/competitorMonitor.ts
autonomous: true
requirements:
  - PORT-09
gap_closure: true

must_haves:
  truths:
    - "scrapeSearchResults returns array of ListingPreview objects with actual listings from search pages"
    - "Each portal's search results page is scraped with portal-specific selectors"
    - "Scraped listings include sourceUrl, title, price, location, and photoUrl"
    - "sourceId is extracted from listing URLs for duplicate detection"
  artifacts:
    - path: "functions/src/schedulers/competitorMonitor.ts"
      provides: "Implemented scrapeSearchResults function"
      contains: "createBrowser"
      min_lines: 350
  key_links:
    - from: "functions/src/schedulers/competitorMonitor.ts"
      to: "functions/src/scrapers/common.ts"
      via: "createBrowser, scrapeWithRetry, extractListingId, normalizePriceText imports"
      pattern: "import.*createBrowser.*from.*common"
    - from: "scrapeSearchResults"
      to: "ListingPreview[]"
      via: "return statement with populated array"
      pattern: "return\\s+listings"
---

<objective>
Implement the scrapeSearchResults function to close the verification gap preventing competitor monitoring from working.

Purpose: The monitoring infrastructure is complete but scrapeSearchResults returns an empty array. This plan implements the actual scraping logic for search result pages across all 3 Turkish real estate portals (sahibinden, hepsiemlak, emlakjet), enabling the system to find and notify users of new competitor listings.

Output: Working scrapeSearchResults function that uses Playwright to scrape search result pages and returns ListingPreview[] for notification creation.
</objective>

<execution_context>
@/home/cagr/.claude/get-shit-done/workflows/execute-plan.md
@/home/cagr/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-background-processing-scraping/03-VERIFICATION.md

# Primary file to modify
@functions/src/schedulers/competitorMonitor.ts

# Existing patterns to follow
@functions/src/scrapers/common.ts
@functions/src/scrapers/sahibinden.ts
@functions/src/scrapers/hepsiemlak.ts
@functions/src/scrapers/emlakjet.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement scrapeSearchResults with portal-specific scrapers</name>
  <files>functions/src/schedulers/competitorMonitor.ts</files>
  <action>
Replace the stub scrapeSearchResults function (lines 261-282) with a full implementation that:

1. **Add imports at top of file:**
   ```typescript
   import { createBrowser, scrapeWithRetry, extractListingId, normalizePriceText, randomDelay } from '../scrapers/common';
   ```

2. **Implement scrapeSearchResults function:**
   - Use scrapeWithRetry wrapper for resilience
   - Call createBrowser() to get Playwright browser and page
   - Navigate to searchUrl with 30s timeout
   - Wait for search results to load (use portal-specific selector)
   - Call portal-specific extraction function based on portal parameter
   - Close browser in finally block
   - Return array of ListingPreview objects

3. **Implement portal-specific extraction functions:**

   **scrapeSahibindenSearchResults(page):**
   - Wait for selector: '.searchResultsItem, [class*="listing-item"], .classified-list tbody tr'
   - Extract listing cards using $$eval
   - For each card extract:
     - title: '.classifiedTitle, [class*="title"] a' textContent
     - sourceUrl: '.classifiedTitle a, [class*="title"] a' href (prepend https://www.sahibinden.com if relative)
     - price: '.price, [class*="price"]' textContent -> normalizePriceText
     - location: '.searchResultsLocationValue, [class*="location"]' textContent
     - photoUrl: 'img' src or data-src
   - Extract sourceId using extractListingId(sourceUrl, 'sahibinden')
   - Return array of ListingPreview with portal: 'sahibinden'

   **scrapeHepsiemlakSearchResults(page):**
   - Wait for selector: '.listing-card, [class*="ListingCard"], .list-view-item'
   - For each card extract:
     - title: '.listing-title, [class*="Title"]' textContent
     - sourceUrl: 'a[href*="/"]' href (prepend https://www.hepsiemlak.com if relative)
     - price: '.listing-price, [class*="Price"]' textContent -> normalizePriceText
     - location: '.listing-location, [class*="Location"]' textContent
     - photoUrl: 'img' src or data-src
   - Extract sourceId, return ListingPreview with portal: 'hepsiemlak'

   **scrapeEmlakjetSearchResults(page):**
   - Wait for selector: '.listing-card, [class*="estate-card"], .listing-item'
   - For each card extract:
     - title: '[class*="title"], h3, h4' textContent
     - sourceUrl: 'a' href (prepend https://www.emlakjet.com if relative)
     - price: '[class*="price"]' textContent -> normalizePriceText
     - location: '[class*="location"], [class*="address"]' textContent
     - photoUrl: 'img' src or data-lazy
   - Extract sourceId, return ListingPreview with portal: 'emlakjet'

4. **Error handling:**
   - Wrap each portal scraper in try/catch
   - Log errors but don't fail entire function
   - Return empty array on unrecoverable errors
   - Limit to first 20 results to avoid overwhelming (take slice)

5. **Rate limiting:**
   - Add randomDelay(1000, 2000) after navigation before extracting

**Important patterns to follow (from existing scrapers):**
- Use `page.waitForSelector()` with 10s timeout before extracting
- Use `page.$$eval()` for batch extraction of listing cards
- Handle data-src and data-lazy attributes for lazy-loaded images
- Filter out data:image URLs from photos
- Trim all text content
- Prepend portal domain to relative URLs
- Close browser in finally block even on error

**CSS selectors rationale:**
These selectors are derived from common patterns in Turkish real estate portals. They use multiple fallbacks (comma-separated) because portal HTML structures may vary. The detail page scrapers in sahibinden.ts, hepsiemlak.ts, and emlakjet.ts use similar selector patterns.
  </action>
  <verify>
Run TypeScript compilation to verify no errors:
```bash
cd functions && npm run build
```

Check that scrapeSearchResults no longer has TODO comment:
```bash
grep -n "TODO" functions/src/schedulers/competitorMonitor.ts
```

Verify imports are added:
```bash
grep -n "createBrowser" functions/src/schedulers/competitorMonitor.ts
```
  </verify>
  <done>
scrapeSearchResults function is implemented with:
- createBrowser/scrapeWithRetry imports from common.ts
- Portal-specific extraction functions for all 3 portals
- Proper error handling and browser cleanup
- Returns ListingPreview[] populated from actual search page HTML
- TypeScript compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Add integration logging and result limiting</name>
  <files>functions/src/schedulers/competitorMonitor.ts</files>
  <action>
Enhance the scrapeSearchResults function with production-ready features:

1. **Add result limiting:**
   - Accept optional `maxResults` parameter (default: 10)
   - Slice results before returning to avoid scraping too many listings
   - This prevents overwhelming the notification system

2. **Add detailed logging:**
   - Log when starting to scrape each portal
   - Log number of listing cards found on page
   - Log number of valid listings extracted (after filtering)
   - Log any selectors that didn't match (for debugging portal changes)

3. **Add empty result detection:**
   - If page loads but no listings found, log warning with URL
   - Check if "no results" message is present (common Turkish patterns: "sonuç bulunamadı", "ilan bulunamadı")
   - Return empty array gracefully

4. **Add URL validation in extracted listings:**
   - Skip listings where sourceUrl is empty or invalid
   - Skip listings where title is empty or just whitespace
   - Log skipped listings count

5. **Update function signature:**
   ```typescript
   async function scrapeSearchResults(
     portal: 'sahibinden' | 'hepsiemlak' | 'emlakjet',
     searchUrl: string,
     maxResults: number = 10
   ): Promise<ListingPreview[]>
   ```
  </action>
  <verify>
Run TypeScript compilation:
```bash
cd functions && npm run build
```

Verify maxResults parameter exists:
```bash
grep -n "maxResults" functions/src/schedulers/competitorMonitor.ts
```

Verify logging is present:
```bash
grep -n "console.log" functions/src/schedulers/competitorMonitor.ts | grep -i "listing\|scrape\|found"
```
  </verify>
  <done>
scrapeSearchResults function enhanced with:
- maxResults parameter (default 10) to limit scraped listings
- Detailed logging for debugging and monitoring
- Empty result detection with appropriate warnings
- URL/title validation to skip invalid listings
- TypeScript compiles without errors
  </done>
</task>

</tasks>

<verification>
After both tasks complete, verify the gap is closed:

1. **Code verification:**
```bash
# Compile functions
cd functions && npm run build

# Verify no TODO comments remain in scrapeSearchResults
grep -A 5 "async function scrapeSearchResults" functions/src/schedulers/competitorMonitor.ts | grep -v "TODO"

# Verify imports from common.ts
grep "import.*createBrowser" functions/src/schedulers/competitorMonitor.ts

# Verify function returns populated array (not empty array literal)
grep -A 50 "async function scrapeSearchResults" functions/src/schedulers/competitorMonitor.ts | grep -v "return \[\]"
```

2. **Key implementation checklist:**
- [ ] scrapeSearchResults uses createBrowser from common.ts
- [ ] scrapeSearchResults uses scrapeWithRetry for resilience
- [ ] Portal-specific extraction for sahibinden, hepsiemlak, emlakjet
- [ ] extractListingId called to populate sourceId
- [ ] normalizePriceText called to parse Turkish price formats
- [ ] Browser closed in finally block
- [ ] Results limited to maxResults (default 10)
- [ ] Detailed logging for debugging
</verification>

<success_criteria>
1. TypeScript compiles without errors (`npm run build` in functions/)
2. scrapeSearchResults function body contains actual scraping logic (not TODO or empty return)
3. All 3 portals have specific extraction logic
4. Function imports and uses createBrowser, scrapeWithRetry, extractListingId from common.ts
5. VERIFICATION.md gap (PORT-09 blocked) can be re-verified as SATISFIED
</success_criteria>

<output>
After completion, create `.planning/phases/03-background-processing-scraping/03-06-SUMMARY.md`
</output>

---
phase: 04-media-enhancement-voice
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - functions/src/voice/transcribeVoice.ts
  - functions/src/index.ts
  - functions/package.json
  - src/services/voiceCommands.ts
  - src/components/voice/VoiceCommandInput.tsx
  - src/components/chat/VoiceButton.tsx
  - src/hooks/useVoiceCommand.ts
autonomous: true
requirements:
  - AIUI-09
  - AIUI-10
user_setup:
  - service: openai
    why: "OpenAI Whisper API for Turkish speech-to-text"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Dashboard -> API keys -> Create new secret key"

must_haves:
  truths:
    - "User can press and hold button to record voice in Turkish"
    - "Recording has visual feedback (pulsing indicator, timer)"
    - "Voice is transcribed to text using OpenAI Whisper"
    - "Transcribed text appears in chat input for confirmation"
    - "User can execute voice command or edit before sending"
    - "Works in all major browsers (Chrome, Firefox, Safari)"
  artifacts:
    - path: "functions/src/voice/transcribeVoice.ts"
      provides: "Whisper API integration Cloud Function"
      min_lines: 60
    - path: "src/services/voiceCommands.ts"
      provides: "Client voice recording and transcription service"
      exports: ["transcribeVoiceCommand", "startRecording", "stopRecording"]
    - path: "src/components/voice/VoiceCommandInput.tsx"
      provides: "Voice input UI with recording feedback"
      min_lines: 80
    - path: "src/hooks/useVoiceCommand.ts"
      provides: "Hook for voice recording state management"
      exports: ["useVoiceCommand"]
  key_links:
    - from: "src/components/voice/VoiceCommandInput.tsx"
      to: "src/hooks/useVoiceCommand.ts"
      via: "uses hook for recording state and transcription"
      pattern: "useVoiceCommand"
    - from: "src/hooks/useVoiceCommand.ts"
      to: "src/services/voiceCommands.ts"
      via: "calls transcription service"
      pattern: "transcribeVoiceCommand"
    - from: "src/services/voiceCommands.ts"
      to: "functions/src/voice/transcribeVoice.ts"
      via: "HTTP callable for Whisper transcription"
      pattern: "httpsCallable.*transcribeVoice"
---

<objective>
Implement Turkish voice command input using OpenAI Whisper API for accurate speech-to-text.

Purpose: Allow users to interact with the AI assistant using voice in Turkish, making the app more accessible and efficient for busy real estate agents.
Output: Whisper-powered transcription Cloud Function, voice recording components, integration with existing chat.
</objective>

<execution_context>
@/home/cagr/.claude/get-shit-done/workflows/execute-plan.md
@/home/cagr/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-media-enhancement-voice/04-RESEARCH.md

# Existing voice/chat components
@src/components/chat/VoiceButton.tsx
@src/hooks/useVoiceInput.ts
@src/components/chat/ChatInput.tsx
@src/components/chat/ChatModal.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Whisper transcription Cloud Function</name>
  <files>
    functions/package.json
    functions/src/voice/transcribeVoice.ts
    functions/src/index.ts
  </files>
  <action>
1. Install OpenAI SDK:
   ```bash
   cd functions && npm install openai
   ```

2. Create functions/src/voice/transcribeVoice.ts:

```typescript
import * as functions from 'firebase-functions/v2/https';
import OpenAI from 'openai';
import * as os from 'os';
import * as path from 'path';
import * as fs from 'fs';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

interface TranscribeRequest {
  audioBase64: string;  // Base64-encoded audio data
  mimeType: string;     // 'audio/webm', 'audio/mp4', etc.
}

interface TranscribeResponse {
  success: boolean;
  transcript?: string;
  error?: string;
  language?: string;
  duration?: number;
}

export const transcribeVoice = functions.onCall(
  {
    region: 'europe-west1',
    memory: '512MiB',
    timeoutSeconds: 60,
  },
  async (request): Promise<TranscribeResponse> => {
    // Verify authentication
    if (!request.auth) {
      throw new functions.HttpsError('unauthenticated', 'User must be logged in');
    }

    const { audioBase64, mimeType } = request.data as TranscribeRequest;

    if (!audioBase64) {
      throw new functions.HttpsError('invalid-argument', 'Audio data required');
    }

    const tempFilePath = path.join(os.tmpdir(), `voice-${Date.now()}.webm`);

    try {
      // Decode base64 and write to temp file
      const audioBuffer = Buffer.from(audioBase64, 'base64');
      fs.writeFileSync(tempFilePath, audioBuffer);

      // Call Whisper API
      const transcription = await openai.audio.transcriptions.create({
        file: fs.createReadStream(tempFilePath),
        model: 'whisper-1',
        language: 'tr',  // Turkish
        response_format: 'verbose_json',  // Get duration and language info
        temperature: 0.2,  // Lower = more conservative
      });

      return {
        success: true,
        transcript: transcription.text,
        language: transcription.language,
        duration: transcription.duration,
      };
    } catch (error) {
      console.error('Transcription error:', error);
      return {
        success: false,
        error: (error as Error).message || 'Transcription failed',
      };
    } finally {
      // Clean up temp file
      if (fs.existsSync(tempFilePath)) {
        fs.unlinkSync(tempFilePath);
      }
    }
  }
);
```

3. Export from index.ts:
```typescript
export { transcribeVoice } from './voice/transcribeVoice';
```

4. Add types file if needed: functions/src/types/voice.ts
  </action>
  <verify>
    - `cd functions && npm run build` passes
    - openai in functions/package.json dependencies
    - transcribeVoice exported from index.ts
    - Function uses Turkish language ('tr')
    - Region set to europe-west1
  </verify>
  <done>
    Whisper transcription Cloud Function ready. Accepts base64 audio, returns Turkish transcript.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create voice command hook and service</name>
  <files>
    src/services/voiceCommands.ts
    src/hooks/useVoiceCommand.ts
  </files>
  <action>
1. Create src/services/voiceCommands.ts:

```typescript
import { getFunctions, httpsCallable } from 'firebase/functions';
import { app } from '../lib/firebase';

const functions = getFunctions(app, 'europe-west1');

interface TranscribeResult {
  success: boolean;
  transcript?: string;
  error?: string;
  duration?: number;
}

export async function transcribeVoiceCommand(audioBlob: Blob): Promise<TranscribeResult> {
  // Convert blob to base64
  const arrayBuffer = await audioBlob.arrayBuffer();
  const base64 = btoa(
    new Uint8Array(arrayBuffer).reduce(
      (data, byte) => data + String.fromCharCode(byte),
      ''
    )
  );

  const transcribeVoice = httpsCallable<
    { audioBase64: string; mimeType: string },
    TranscribeResult
  >(functions, 'transcribeVoice');

  const result = await transcribeVoice({
    audioBase64: base64,
    mimeType: audioBlob.type,
  });

  return result.data;
}
```

2. Create src/hooks/useVoiceCommand.ts:

```typescript
import { useState, useRef, useCallback } from 'react';
import { transcribeVoiceCommand } from '../services/voiceCommands';

interface UseVoiceCommandReturn {
  isRecording: boolean;
  isTranscribing: boolean;
  recordingDuration: number;
  error: string | null;
  transcript: string | null;
  startRecording: () => Promise<void>;
  stopRecording: () => Promise<void>;
  cancelRecording: () => void;
  clearTranscript: () => void;
}

export function useVoiceCommand(): UseVoiceCommandReturn {
  const [isRecording, setIsRecording] = useState(false);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [recordingDuration, setRecordingDuration] = useState(0);
  const [error, setError] = useState<string | null>(null);
  const [transcript, setTranscript] = useState<string | null>(null);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const streamRef = useRef<MediaStream | null>(null);

  const startRecording = useCallback(async () => {
    try {
      setError(null);
      setTranscript(null);
      chunksRef.current = [];

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 16000,
        },
      });
      streamRef.current = stream;

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus',
      });

      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };

      mediaRecorder.start(100);  // Collect in 100ms chunks
      mediaRecorderRef.current = mediaRecorder;
      setIsRecording(true);

      // Start timer
      let seconds = 0;
      timerRef.current = setInterval(() => {
        seconds++;
        setRecordingDuration(seconds);

        // Auto-stop after 60 seconds (Whisper limit)
        if (seconds >= 60) {
          stopRecording();
        }
      }, 1000);
    } catch (err) {
      setError('Mikrofon izni reddedildi');
    }
  }, []);

  const stopRecording = useCallback(async () => {
    if (!mediaRecorderRef.current) return;

    // Clear timer
    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }

    // Stop recording
    return new Promise<void>((resolve) => {
      const mediaRecorder = mediaRecorderRef.current!;

      mediaRecorder.onstop = async () => {
        setIsRecording(false);
        setIsTranscribing(true);

        // Stop all tracks
        streamRef.current?.getTracks().forEach((track) => track.stop());

        try {
          const audioBlob = new Blob(chunksRef.current, { type: 'audio/webm' });
          const result = await transcribeVoiceCommand(audioBlob);

          if (result.success && result.transcript) {
            setTranscript(result.transcript);
          } else {
            setError(result.error || 'Ses tanıma başarısız');
          }
        } catch (err) {
          setError('Ses tanıma hatası');
        } finally {
          setIsTranscribing(false);
          resolve();
        }
      };

      mediaRecorder.stop();
    });
  }, []);

  const cancelRecording = useCallback(() => {
    if (timerRef.current) {
      clearInterval(timerRef.current);
    }
    if (mediaRecorderRef.current?.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
    streamRef.current?.getTracks().forEach((track) => track.stop());
    setIsRecording(false);
    setRecordingDuration(0);
    chunksRef.current = [];
  }, []);

  const clearTranscript = useCallback(() => {
    setTranscript(null);
    setError(null);
    setRecordingDuration(0);
  }, []);

  return {
    isRecording,
    isTranscribing,
    recordingDuration,
    error,
    transcript,
    startRecording,
    stopRecording,
    cancelRecording,
    clearTranscript,
  };
}
```
  </action>
  <verify>
    - `npm run build` passes
    - voiceCommands.ts exports transcribeVoiceCommand
    - useVoiceCommand hook exports all required functions
    - Hook handles recording, transcription, and error states
  </verify>
  <done>
    Voice command hook and service ready. Handles recording, transcription via Whisper, and state management.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create VoiceCommandInput component and integrate with chat</name>
  <files>
    src/components/voice/VoiceCommandInput.tsx
    src/components/chat/VoiceButton.tsx
    src/components/chat/ChatInput.tsx
  </files>
  <action>
1. Create src/components/voice/VoiceCommandInput.tsx:

```typescript
interface VoiceCommandInputProps {
  onTranscript: (text: string) => void;
  disabled?: boolean;
}
```

Features:
- Hold-to-record button with microphone icon
- Visual recording indicator:
  - Pulsing red circle when recording
  - Timer showing recording duration (e.g., "0:05")
  - Waveform visualization (optional, CSS animation)
- Turkish text: "Basılı tutarak konuşun" (Hold to speak)
- Processing state: spinner with "Tanınıyor..." (Recognizing...)
- On transcript: call onTranscript callback
- Error state with retry option

2. Update VoiceButton.tsx (existing component):
- Replace Web Speech API with useVoiceCommand hook
- Keep same UI but use Whisper backend
- Remove Chrome-specific code
- Add fallback message for browsers without MediaRecorder

3. Update ChatInput.tsx:
- When voice transcript received:
  - Insert into input field
  - Focus input for user to confirm/edit
  - Show "Sesli komut tanındı" toast
- User can then press Enter to send or edit first

4. Styling:
- Recording button: large, centered, red pulsing border when active
- Timer: monospace font, positioned above button
- Transcription result: appears in input with highlight animation
  </action>
  <verify>
    - `npm run build` passes
    - VoiceCommandInput renders with microphone button
    - Recording shows pulsing indicator and timer
    - Transcription appears in chat input
    - Works in Firefox and Safari (not just Chrome)
  </verify>
  <done>
    Voice command input integrated with chat. Users can record voice in Turkish, see transcription, and send as command.
  </done>
</task>

</tasks>

<verification>
1. Build passes: `npm run build` and `cd functions && npm run build`
2. Set OPENAI_API_KEY in Firebase Functions config
3. Deploy: `firebase deploy --only functions`
4. Open chat, click voice button
5. Hold button and say "Yeni mulk ekle" in Turkish
6. Release button, see processing indicator
7. Turkish transcript appears in input field
8. Press Enter to execute command
9. Test in Firefox to verify cross-browser support
</verification>

<success_criteria>
- Voice recording works with hold-to-speak interaction
- Visual feedback shows recording state and duration
- Whisper API accurately transcribes Turkish speech
- Transcript appears in chat input for confirmation
- User can edit before sending
- Works in Chrome, Firefox, and Safari
- 60-second recording limit enforced
- Error states show helpful Turkish messages
</success_criteria>

<output>
After completion, create `.planning/phases/04-media-enhancement-voice/04-04-SUMMARY.md`
</output>
